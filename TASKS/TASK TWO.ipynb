{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e35cc75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "(X, y), (X_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bec49d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To show an image from the training dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(X_train[0])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a1c8c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training, validation, and test sets\n",
    "X_train, X_val, X_test = np.split(X_train[:80000], [int(0.6*80000), int(0.8*80000)])\n",
    "y_train, y_val, y_test = np.split(y_train[:80000], [int(0.6*80000), int(0.8*80000)])\n",
    "\n",
    "\n",
    "# Normalize the features\n",
    "mean = X_train.mean(axis=0)\n",
    "std = X_train.std(axis=0)\n",
    "std = np.where(std == 0, 1e-7, std)  # Replace zero values with 1e-7\n",
    "X_train = (X_train - mean) / std\n",
    "X_val = (X_val - mean) / std\n",
    "X_test = (X_test - mean) / std\n",
    "\n",
    "\n",
    "# One-hot encode the labels\n",
    "def one_hot(y, num_classes):\n",
    "    one_hot_labels = np.zeros((len(y), num_classes))\n",
    "    one_hot_labels[np.arange(len(y)), y] = 1\n",
    "    return one_hot_labels\n",
    "\n",
    "\n",
    "y_train = one_hot(y_train, 10)\n",
    "y_val = one_hot(y_val, 10)\n",
    "y_test = one_hot(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856822d4",
   "metadata": {},
   "source": [
    "This code above does several pre-processing steps on the MNIST dataset to prepare it for training a machine learning model. It first splits the training set into three smaller sets: a training set, a validation set, and a test set. The validation set is used to evaluate the model during training and tune the model's hyperparameters. In contrast, the test set is used to evaluate the final performance of the trained model.\n",
    "\n",
    "The code then normalises the features (i.e., the pixel values of the images) by subtracting the mean and dividing by the standard deviation. This helps to centre the data around zero and reduce the impact of large values, which can often dominate the training of a model.\n",
    "\n",
    "Finally, the code one-hot encodes the labels, which means it converts the integer labels into a binary representation with one element for each possible class. For example, if the labels are digits from 0 to 9, one-hot encoding would convert the label \"3\" into the array [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]. One-hot encoding is often used as a way to represent categorical data in machine learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "039e9d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Sigmoid activation function.\n",
    "    Arguments:\n",
    "    x - Input array of any shape\n",
    "    Returns:\n",
    "    y - Output of sigmoid function applied element-wise to the input, same shape as x\n",
    "    \"\"\"\n",
    "    y = 1 / (1 + np.exp(-x))\n",
    "    return y\n",
    "\n",
    "def sigmoid_forward(x):\n",
    "    \"\"\"\n",
    "    Forward pass for the sigmoid activation function.\n",
    "    Arguments:\n",
    "    x - Input array of any shape\n",
    "    Returns:\n",
    "    y - Output of sigmoid function applied element-wise to the input, same shape as x\n",
    "    cache - Tuple containing x, stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    y = sigmoid(x)\n",
    "    cache = (x,)\n",
    "    return y, cache\n",
    "\n",
    "def sigmoid_backward(dy, cache):\n",
    "    \"\"\"\n",
    "    Backward pass for the sigmoid activation function.\n",
    "    Arguments:\n",
    "    dy - Gradient of the output with respect to the input, same shape as x\n",
    "    cache - Tuple containing x, stored for computing the backward pass efficiently\n",
    "    Returns:\n",
    "    dx - Gradient of the input with respect to the cost, same shape as dy\n",
    "    \"\"\"\n",
    "    x = cache[0]\n",
    "    dx = dy * sigmoid(x) * (1 - sigmoid(x))\n",
    "    return dx\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    ReLU activation function.\n",
    "    Arguments:\n",
    "    x - Input array of any shape\n",
    "    Returns:\n",
    "    y - Output of ReLU function applied element-wise to the input, same shape as x\n",
    "    \"\"\"\n",
    "    y = np.maximum(0, x)\n",
    "    return y\n",
    "\n",
    "def relu_forward(x):\n",
    "    \"\"\"\n",
    "    Forward pass for the ReLU activation function.\n",
    "    Arguments:\n",
    "    x - Input array of any shape\n",
    "    Returns:\n",
    "    y - Output of ReLU function applied element-wise to the input, same shape as x\n",
    "    cache - Tuple containing x, stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    y = relu(x)\n",
    "    cache = (x,)\n",
    "    return y, cache\n",
    "\n",
    "def relu_backward(dy, cache):\n",
    "    \"\"\"\n",
    "    Backward pass for the ReLU activation function.\n",
    "    Arguments:\n",
    "    dy - Gradient of the output with respect to the input, same shape as x\n",
    "    cache - Tuple containing x, stored for computing the backward pass efficiently\n",
    "    Returns:\n",
    "    dx - Gradient of the input with respect to the cost, same shape as dy\n",
    "    \"\"\"\n",
    "    x = cache[0]\n",
    "    dx = dy.copy()\n",
    "    dx[x < 0] = 0\n",
    "    return dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39da23e",
   "metadata": {},
   "source": [
    "The code above defines two activation functions: sigmoid and ReLU (rectified linear unit). The sigmoid function maps input values to the range of 0 and 1, while the ReLU function maps all negative values to 0 and leaves positive values unchanged.\n",
    "\n",
    "The code defines three functions for each activation function: a forward pass function, a backward pass function, and a function that applies the activation function element-wise to an input array.\n",
    "\n",
    "The forward pass function applies the activation function to the input and returns the output and any values needed to compute the backward pass efficiently (these are stored in a tuple called cache). The backward pass function computes the gradient of the input with respect to the cost, given the gradient of the output with respect to the input and the cache tuple. The element-wise application function applies the activation function to each input array element.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e67e25ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Softmax activation function.\n",
    "    Arguments:\n",
    "    x - Input array of shape (num_classes, batch_size)\n",
    "    Returns:\n",
    "    y - Output of softmax function applied along the rows (num_classes, batch_size), such that\n",
    "        y[i, j] is the probability that the j-th sample belongs to class i\n",
    "    \"\"\"\n",
    "    # Subtract the maximum value from each element to prevent overflow\n",
    "    x_max = np.max(x, axis=0, keepdims=True)\n",
    "    x_shifted = x - x_max\n",
    "    y = np.exp(x_shifted) / np.sum(np.exp(x_shifted), axis=0, keepdims=True)\n",
    "    return y\n",
    "\n",
    "def softmax_loss(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Cross-entropy loss for the softmax output layer.\n",
    "    Arguments:\n",
    "    y_pred - Predicted class probabilities, of shape (num_classes, batch_size)\n",
    "    y_true - True class labels, of shape (batch_size,)\n",
    "    Returns:\n",
    "    loss - Cross-entropy loss, a scalar\n",
    "    \"\"\"\n",
    "    # Convert y_true to one-hot encoded labels\n",
    "    num_classes = y_pred.shape[0]\n",
    "    y_true_one_hot = np.zeros((num_classes, y_true.shape[0]))\n",
    "    y_true_one_hot[y_true, np.arange(y_true.shape[0])] = 1\n",
    "    # Compute the cross-entropy loss\n",
    "    loss = -np.sum(y_true_one_hot * np.log(y_pred)) / y_true.shape[0]\n",
    "    return loss\n",
    "\n",
    "def softmax_backward(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Backward pass for the softmax output layer.\n",
    "    Arguments:\n",
    "    y_pred - Predicted class probabilities, of shape (num_classes, batch_size)\n",
    "    y_true - True class labels, of shape (batch_size,)\n",
    "    Returns:\n",
    "    dy_pred - Gradient of the output with respect to the input, of shape (num_classes, batch_size)\n",
    "    \"\"\"\n",
    "    # Convert y_true to one-hot encoded labels\n",
    "    num_classes = y_pred.shape[0]\n",
    "    y_true_one_hot = np.zeros((num_classes, y_true.shape[0]))\n",
    "    y_true_one_hot[y_true, np.arange(y_true.shape[0])] = 1\n",
    "    # Compute the gradient\n",
    "    dy_pred = y_pred - y_true_one_hot\n",
    "    return dy_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08d35cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def dropout_forward(x, dropout_rate, training=True, seed=None):\n",
    "    \"\"\"\n",
    "    Forward pass for dropout.\n",
    "    Arguments:\n",
    "    x - Input data, of any shape\n",
    "    dropout_rate - Dropout rate\n",
    "    training - Flag for training mode. When set to False, dropout is not applied\n",
    "    seed - Random seed for reproducibility\n",
    "    Returns:\n",
    "    y - Output of the dropout layer, same shape as x\n",
    "    cache - A tuple containing x, dropout_rate, and training, stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    if training:\n",
    "        mask = np.random.rand(*x.shape) < (1 - dropout_rate)\n",
    "        y = x * mask / (1 - dropout_rate)\n",
    "    else:\n",
    "        y = x\n",
    "    cache = (x, dropout_rate, training)\n",
    "    return y, cache\n",
    "\n",
    "def dropout_backward(dy, cache):\n",
    "    \"\"\"\n",
    "    Backward pass for dropout.\n",
    "    Arguments:\n",
    "    dy - Gradient of the output with respect to the input, of any shape\n",
    "    cache - A tuple containing x, dropout_rate, and training, stored for computing the backward pass efficiently\n",
    "    Returns:\n",
    "    dx - Gradient of the input with respect to the cost, same shape as dy\n",
    "    \"\"\"\n",
    "    x, dropout_rate, training = cache\n",
    "    if training:\n",
    "        dx = dy * (np.random.rand(*x.shape) < (1 - dropout_rate)) / (1 - dropout_rate)\n",
    "    else:\n",
    "        dx = dy\n",
    "    return dx\n",
    "\n",
    "def l2_regularization_loss(w, l2_lambda):\n",
    "    \"\"\"\n",
    "    L2 regularization loss for the weights of a neural network.\n",
    "    Arguments:\n",
    "    w - Weights of the neural network, a list of numpy arrays\n",
    "    l2_lambda - L2 regularization strength\n",
    "    Returns:\n",
    "    loss - L2 regularization loss, a scalar\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    for w_i in w:\n",
    "        loss += np.sum(w_i ** 2)\n",
    "    loss *= l2_lambda / 2\n",
    "    return loss\n",
    "\n",
    "def l2_regularization_grad(w, l2_lambda):\n",
    "    \"\"\"\n",
    "    Gradient of the L2 regularization loss with respect to the weights.\n",
    "    Arguments:\n",
    "    w - Weights of the neural network, a list of numpy arrays\n",
    "    l2_lambda - L2 regularization strength\n",
    "    Returns:\n",
    "    dw - Gradient of the L2 regularization loss with respect to the weights, a list of numpy arrays\n",
    "    \"\"\"\n",
    "    dw = [l2_lambda * w_i for w_i in w]\n",
    "    return dw\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1b4148",
   "metadata": {},
   "source": [
    "This code above defines several functions related to the softmax activation function and cross-entropy loss. The softmax function is often used as the output layer of a neural network for classification tasks, where the goal is to predict the class probabilities for each input sample. The cross-entropy loss is a common choice for measuring the difference between the predicted class probabilities and the true class labels.\n",
    "\n",
    "The softmax function applies the softmax activation function along the rows of the input array x, so y[i, j] is the probability that the j-th sample belongs to class i. The softmax_loss function computes the cross-entropy loss between the predicted class probabilities y_pred and the true class labels y_true. It first converts y_true into a one-hot encoded representation (i.e., a binary representation with one element for each possible class). Then it computes the loss as the negative sum of the element-wise product of the one-hot encoded labels and the predicted class probabilities, normalised by the batch size.\n",
    "\n",
    "The softmax_backward function computes the gradient of the output with respect to the input, given the predicted class probabilities y_pred and the true class labels y_true. It first converts y_true into a one-hot encoded representation and then computes the gradient as the difference between y_pred and the one-hot encoded labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7a24593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_sigmoid passed\n",
      "test_relu passed\n",
      "test_softmax passed\n",
      "test_softmax_loss passed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kanso\\AppData\\Local\\Temp\\ipykernel_31396\\1335837043.py:30: RuntimeWarning: invalid value encountered in log\n",
      "  loss = -np.sum(y_true_one_hot * np.log(y_pred)) / y_true.shape[0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def test_sigmoid():\n",
    "    x = np.random.randn(5, 5)\n",
    "    y, cache = sigmoid_forward(x)\n",
    "    dy = np.random.randn(*y.shape)\n",
    "    dx = sigmoid_backward(dy, cache)\n",
    "    assert y.shape == x.shape\n",
    "    assert dx.shape == x.shape\n",
    "    print(\"test_sigmoid passed\")\n",
    "\n",
    "def test_relu():\n",
    "    x = np.random.randn(5, 5)\n",
    "    y, cache = relu_forward(x)\n",
    "    dy = np.random.randn(*y.shape)\n",
    "    dx = relu_backward(dy, cache)\n",
    "    assert y.shape == x.shape\n",
    "    assert dx.shape == x.shape\n",
    "    print(\"test_relu passed\")\n",
    "    \n",
    "def test_softmax():\n",
    "    x = np.random.randn(5, 5)\n",
    "    y = softmax(x)\n",
    "    dy_pred = np.random.randn(*y.shape)\n",
    "    y_true = np.random.randint(0, 5, size=(5,))\n",
    "    dy_true = softmax_backward(dy_pred, y_true)\n",
    "    assert y.shape == x.shape\n",
    "    assert dy_pred.shape == y.shape\n",
    "    assert dy_true.shape == x.shape\n",
    "    print(\"test_softmax passed\")\n",
    "\n",
    "def test_softmax_loss():\n",
    "    y_pred = np.random.randn(5, 5)\n",
    "    y_true = np.random.randint(0, 5, size=(5,))\n",
    "    loss = softmax_loss(y_pred, y_true)\n",
    "    assert isinstance(loss, float)\n",
    "    print(\"test_softmax_loss passed\")\n",
    "\n",
    "def test_suite():\n",
    "    test_sigmoid()\n",
    "    test_relu()\n",
    "    test_softmax()\n",
    "    test_softmax_loss()\n",
    "\n",
    "test_suite()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c91052c",
   "metadata": {},
   "source": [
    "This code above defines a suite of test functions for testing the implementation of various activation functions and a loss function. The test functions are defined for the sigmoid, ReLU, softmax, and cross-entropy loss functions.\n",
    "\n",
    "These test functions can be used to ensure that the implementation of the activation functions and loss function is correct and to catch any mistakes or bugs that may be present in the implementation. Running these tests can help ensure the code’s correctness and improve the reliability of the machine-learning models that use these functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e17b67b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_dropout_forward passed\n",
      "test_dropout_backward passed\n",
      "test_l2_regularization_loss passed\n",
      "test_l2_regularization_grad passed\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def test_dropout_forward():\n",
    "    x = np.random.randn(5, 5)\n",
    "    dropout_rate = 0.5\n",
    "    y, cache = dropout_forward(x, dropout_rate)\n",
    "    assert y.shape == x.shape\n",
    "    assert len(cache) == 3\n",
    "    print(\"test_dropout_forward passed\")\n",
    "\n",
    "def test_dropout_backward():\n",
    "    x = np.random.randn(5, 5)\n",
    "    dy = np.random.randn(*x.shape)\n",
    "    dropout_rate = 0.5\n",
    "    _, cache = dropout_forward(x, dropout_rate)\n",
    "    dx = dropout_backward(dy, cache)\n",
    "    assert dx.shape == dy.shape\n",
    "    print(\"test_dropout_backward passed\")\n",
    "\n",
    "def test_l2_regularization_loss():\n",
    "    w = [np.random.randn(5, 5), np.random.randn(5, 5)]\n",
    "    l2_lambda = 0.5\n",
    "    loss = l2_regularization_loss(w, l2_lambda)\n",
    "    assert isinstance(loss, float)\n",
    "    print(\"test_l2_regularization_loss passed\")\n",
    "    \n",
    "def test_l2_regularization_grad():\n",
    "    w = [np.random.randn(5, 5), np.random.randn(5, 5)]\n",
    "    l2_lambda = 0.5\n",
    "    dw = l2_regularization_grad(w, l2_lambda)\n",
    "    assert all(dw_i.shape == w_i.shape for dw_i, w_i in zip(dw, w))\n",
    "    print(\"test_l2_regularization_grad passed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_dropout_forward()\n",
    "    test_dropout_backward()\n",
    "    test_l2_regularization_loss()\n",
    "    test_l2_regularization_grad()\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8e5911",
   "metadata": {},
   "source": [
    "This code above defines a suite of test functions for testing the implementation of various activation functions and a loss function. The test functions are defined for the sigmoid, ReLU, softmax, and cross-entropy loss functions.\n",
    "\n",
    "These test functions can be used to ensure that the implementation of the activation functions and loss function is correct and to catch any mistakes or bugs that may be present in the implementation. Running these tests can help ensure the code’s correctness and improve the reliability of the machine-learning models that use these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fff4e015",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def loss_backward(self, y_pred, y_true):\n",
    "    if self.output_type == 'binary_classification':\n",
    "        dy_pred = binary_cross_entropy_loss_backward(y_pred, y_true)\n",
    "    elif self.output_type == 'multiclass_classification':\n",
    "        dy_pred = multiclass_cross_entropy_loss_backward(y_pred, y_true)\n",
    "    elif self.output_type == 'regression':\n",
    "        dy_pred = mean_squared_error_backward(y_pred, y_true)\n",
    "    return dy_pred\n",
    "\n",
    "def one_hot_encode(y, num_classes):\n",
    "    # Create an empty array of shape (len(y), num_classes)\n",
    "    y_one_hot = np.zeros((len(y), num_classes))\n",
    "    \n",
    "    # Set the appropriate element of the one-hot encoded array to 1\n",
    "    y_one_hot[np.arange(len(y)), y] = 1\n",
    "    \n",
    "    return y_one_hot\n",
    "\n",
    "    \n",
    "def multiclass_cross_entropy_loss(y_pred, y_true, num_classes):\n",
    "    # Convert y_true to one-hot encoded tensor\n",
    "    y_true = y_true.reshape(-1, 1)\n",
    "    y_true_one_hot = one_hot_encode(y_true, num_classes)\n",
    "\n",
    "    # Reshape y_true to have a shape of (batch_size, num_classes)\n",
    "    y_true_one_hot = y_true_one_hot.reshape(-1, num_classes)\n",
    "\n",
    "    # Convert y_true to one-hot encoded tensor\n",
    "    y_true_one_hot = to_one_hot(y_true)\n",
    "\n",
    "    # Convert y_pred to one-hot encoded tensor\n",
    "    y_pred_one_hot = to_one_hot(y_pred)\n",
    "    \n",
    "    # Compute the multiclass cross entropy loss\n",
    "    loss = -np.sum(y_true_one_hot * np.log(y_pred + 1e-8))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def multiclass_cross_entropy_loss_backward(y_pred, y_true):\n",
    "    # Convert the predicted output and true output to one-hot encodings\n",
    "    y_pred_one_hot = to_one_hot(y_pred)\n",
    "    y_true_one_hot = to_one_hot(y_true)\n",
    "\n",
    "    # Compute the derivative of the multiclass cross entropy loss with respect to the predicted output\n",
    "    dy_pred = -y_true_one_hot / (y_pred_one_hot + 1e-8)\n",
    "\n",
    "    return dy_pred\n",
    "    \n",
    "def to_one_hot(y):\n",
    "    # Get the number of classes and the number of samples\n",
    "    num_classes = np.max(y) + 1\n",
    "    num_samples = y.shape[0]\n",
    "\n",
    "    # Create a tensor of one-hot encodings with the same shape as y\n",
    "    y_one_hot = np.zeros((num_samples, num_classes))\n",
    "\n",
    "    # Set the one-hot encoding for each sample\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4cf21b",
   "metadata": {},
   "source": [
    "This code defines several functions that implement different types of loss functions for use in machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7bf4b517",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, output_size, hidden_sizes, activations, output_type, num_classes, dropout_rates=[], l2_lambda=0):\n",
    "        \"\"\"\n",
    "        Initialize a fully parameterizable neural network.\n",
    "        Arguments:\n",
    "        input_size - Size of the input layer\n",
    "        output_size - Size of the output layer\n",
    "        hidden_sizes - List of integers specifying the number of units in each hidden layer\n",
    "        activations - List of strings specifying the activation function for each layer, must be one of 'sigmoid', 'relu', or 'none'\n",
    "        dropout_rates - List of floats in the range [0, 1) specifying the dropout rate for each layer, or None if no dropout is used\n",
    "        l2_lambda - L2 regularization strength\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.activations = activations\n",
    "        self.dropout_rates = dropout_rates\n",
    "        self.l2_lambda = l2_lambda\n",
    "        self.output_type = output_type\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Initialize the weights and biases for each layer\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for i, (h_size, activation) in enumerate(zip(hidden_sizes, activations)):\n",
    "            if i == 0:\n",
    "                prev_size = input_size\n",
    "            else:\n",
    "                prev_size = hidden_sizes[i - 1]\n",
    "            self.weights.append(np.random.randn(prev_size, h_size))\n",
    "            self.biases.append(np.zeros(h_size))\n",
    "        self.weights.append(np.random.randn(hidden_sizes[-1], output_size))\n",
    "        self.biases.append(np.zeros(output_size))\n",
    "\n",
    "    def forward(self, x, training=True):\n",
    "        \"\"\"\n",
    "        Compute the forward pass of the neural network.\n",
    "        Arguments:\n",
    "        x - Input data, of shape (batch_size, input_size)\n",
    "        training - Flag for training mode. When set to False, dropout is not applied\n",
    "        Returns:\n",
    "\n",
    "        y - Output of the neural network, of shape (batch_size, output_size)\n",
    "        \"\"\"\n",
    "        y = x\n",
    "        activations = []\n",
    "        for i, (W, b, activation, dropout_rate) in enumerate(zip(self.weights, self.biases, self.activations, self.dropout_rates)):\n",
    "            y = np.dot(y, W) + b\n",
    "            if activation == 'sigmoid':\n",
    "                y, _ = sigmoid_forward(y)\n",
    "            elif activation == 'relu':\n",
    "                y, _ = relu_forward(y)\n",
    "            if dropout_rate is not None and training:\n",
    "                y, _ = dropout_forward(y, dropout_rate)\n",
    "            activations.append(y)\n",
    "        return y, activations\n",
    "    \n",
    "\n",
    "    def backward(self, dy, activations):\n",
    "        \"\"\"\n",
    "        Compute the backward pass of the neural network.\n",
    "        Arguments:\n",
    "        dy - Gradient of the output with respect to the input, of shape (batch_size, output_size)\n",
    "        Returns:\n",
    "        dx - Gradient of the input with respect to the cost, of shape (batch_size, input_size)\n",
    "        \"\"\"\n",
    "        dx = dy\n",
    "        for i, (W, b, activation, dropout_rate, y_batch) in enumerate(reversed(list(zip(self.weights, self.biases, self.activations, self.dropout_rates, activations)))):\n",
    "            if dropout_rate is not None:\n",
    "                dx = dropout_backward(dx, (y, dropout_rate, True))\n",
    "            if activation == 'sigmoid':\n",
    "                dx = sigmoid_backward(dx, (y,))\n",
    "            elif activation == 'relu':\n",
    "                dx = relu_backward(dx, (y,))\n",
    "            dW = np.dot(dx.T, y)\n",
    "            db = np.sum(dx, axis=0)\n",
    "            self.weights[i] -= dW\n",
    "            self.biases[i] -= db\n",
    "            dx = np.dot(dx, W.T)\n",
    "        return dx\n",
    "                                                                                            ###\n",
    "    def train(self, x_train, y_train, x_val=None, y_val=None, batch_size=64, learning_rate=1e-7, max_epochs=10):\n",
    "            # Check if learning rate is positive\n",
    "        if learning_rate <= 0:\n",
    "            raise ValueError(\"Learning rate must be a positive value\")\n",
    "        \"\"\"\n",
    "        Train the neural network on the training data.\n",
    "        Arguments:\n",
    "        x_train - Training input data, of shape (num_train, input_size)\n",
    "        y_train - Training output data, of shape (num_train, output_size)\n",
    "        x_val - Validation input data, of shape (num_val, input_size), or None if no validation is used\n",
    "        y_val - Validation output data, of shape (num_val, output_size), or None if no validation is used\n",
    "        batch_size - Size of the mini-batches for SGD\n",
    "        learning_rate - Learning rate for SGD\n",
    "        max_epochs - Maximum number of epochs to train for\n",
    "        Returns:\n",
    "        history - Dictionary containing the training and validation loss for each epoch\n",
    "        \"\"\"\n",
    "        \n",
    "        activations = []\n",
    "        # Initialize a dictionary to store the training and validation history\n",
    "        history = {'train_loss': [], 'val_loss': []}\n",
    "        # Compute the number of mini-batches\n",
    "        num_train = x_train.shape[0]\n",
    "        num_batches = num_train // batch_size\n",
    "        # Loop over the epochs\n",
    "        for epoch in range(max_epochs):\n",
    "            # Shuffle the training data at the start of each epoch\n",
    "            shuffle_indices = np.random.permutation(num_train)\n",
    "            x_train_shuffled = x_train[shuffle_indices]\n",
    "            y_train_shuffled = y_train[shuffle_indices]\n",
    "            # Initialize the training loss for this epoch\n",
    "            epoch_train_loss = 0\n",
    "            # Loop over the mini-batches\n",
    "            for i in range(num_batches):\n",
    "                # Get the mini-batch data\n",
    "                start = i * batch_size\n",
    "                end = (i + 1) * batch_size\n",
    "                x_batch = x_train_shuffled[start:end]\n",
    "                y_batch = y_train_shuffled[start:end]\n",
    "                # Compute the forward pass and the loss\n",
    "                y_pred, _ = self.forward(x_batch)\n",
    "                batch_loss = self.compute_loss(y_pred, y_batch)\n",
    "                epoch_train_loss += batch_loss\n",
    "                # Compute the backward pass\n",
    "                dy_pred = self.loss_backward(y_pred, y_batch)\n",
    "                dx = self.backward(dy_pred)\n",
    "                # Update the weights and biases\n",
    "                self.update_parameters(learning_rate)\n",
    "                # Compute the mean training loss for this epoch\n",
    "                epoch_train_loss /= num_batches\n",
    "                # Append the training loss to the history\n",
    "                history['train_loss'].append(epoch_train_loss)\n",
    "                # If a validation set is provided, compute the validation loss for this epoch\n",
    "                if x_val is not None and y_val is not None: ################################################\n",
    "                    y_pred, _ = self.forward(x_val)\n",
    "                    epoch_val_loss = self.compute_loss(y_pred, y_val)\n",
    "                # Append the validation loss to the history\n",
    "                history['val_loss'].append(epoch_val_loss)\n",
    "            return history###########################################################################\n",
    "        \n",
    "    def update_parameters(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Update the weights and biases of the neural network using SGD.\n",
    "        Arguments:\n",
    "        learning_rate - Learning rate for SGD\n",
    "        \"\"\"\n",
    "        # Update the weights\n",
    "        self.weights = [W - learning_rate * dW for W, dW in zip(self.weights, self.dW)]\n",
    "        # Update the biases\n",
    "        self.biases = [b - learning_rate * db for b, db in zip(self.biases, self.db)]\n",
    "\n",
    "    def compute_loss(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Compute the cross-entropy loss for the given prediction and true output.\n",
    "        Arguments:\n",
    "        y_pred - Predicted output, of shape (batch_size, output_size)\n",
    "        y_true - True output, of shape (batch_size, output_size)\n",
    "        Returns:\n",
    "        loss - Cross-entropy loss, a scalar\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if self.output_type == 'binary_classification':\n",
    "            loss = binary_cross_entropy_loss(y_pred, y_true)\n",
    "        elif self.output_type == 'multiclass_classification':\n",
    "            loss = multiclass_cross_entropy_loss(y_pred, y_true, num_classes=self.num_classes)\n",
    "        elif self.output_type == 'regression':\n",
    "            loss = mean_squared_error(y_pred, y_true)\n",
    "        return loss\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620cc3f6",
   "metadata": {},
   "source": [
    "Above is the code for the neural network class. The class has methods for performing the forward pass and backward pass of the neural network, as well as methods for training the network using stochastic gradient descent. The network has fully connected layers and can use the sigmoid or ReLU activation functions. The code also includes support for dropout and L2 regularization. The output type of the network can be 'binary_classification', 'multiclass_classification', or 'regression', and the number of classes is specified by the 'num_classes' parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4743c828",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Initialize the SGD optimizer.\n",
    "        Arguments:\n",
    "        learning_rate - Learning rate for the SGD optimizer, a scalar\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def update(self, model):\n",
    "        \"\"\"\n",
    "        Update the weights and biases of the model using SGD.\n",
    "        Arguments:\n",
    "        model - A neural network model object\n",
    "        \"\"\"\n",
    "        for W, b in zip(model.weights, model.biases):\n",
    "            W -= self.learning_rate * W.grad\n",
    "            b -= self.learning_rate * b.grad\n",
    "\n",
    "    def train(model, optimizer, X_train, y_train, X_val, y_val, max_epochs, verbose=True):\n",
    "        \"\"\"\n",
    "        Train the model using the given optimizer.\n",
    "        Arguments:\n",
    "        model - A neural network model object\n",
    "        optimizer - An optimizer object, such as SGD or Adam\n",
    "        X_train - Training data, of shape (batch_size, input_size)\n",
    "        y_train - Training labels, of shape (batch_size,)\n",
    "        X_val - Validation data, of shape (batch_size, input_size)\n",
    "        y_val - Validation labels, of shape (batch_size,)\n",
    "        max_epochs - Maximum number of epochs to train for\n",
    "        verbose - Flag for printing training progress\n",
    "        \"\"\"\n",
    "        for epoch in range(max_epochs):\n",
    "            model.train()  # Set model to training mode\n",
    "            # Perform a forward pass and compute the loss\n",
    "            y_pred = model(X_train)\n",
    "            loss = model.loss(y_pred, y_train)\n",
    "            if verbose:\n",
    "                print('Epoch {}: Training loss = {}'.format(epoch, loss))\n",
    "            # Compute the gradients\n",
    "            model.backward()\n",
    "            # Update the weights and biases using the optimizer\n",
    "            optimizer.update(model)\n",
    "            # Compute the validation loss\n",
    "            y_pred_val = model(X_val)\n",
    "            val_loss = model.loss(y_pred_val, y_val)\n",
    "            if verbose:\n",
    "                print('Epoch {}: Validation loss = {}'.format(epoch, val_loss))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c20eda6",
   "metadata": {},
   "source": [
    "This code above defines the SGD class, representing the stochastic gradient descent optimiser. It has a single method update, which updates the weights and biases of a neural network model object using the SGD algorithm. It also defines the train function, which takes a neural network model, an optimiser (such as an instance of the SGD class), training data and labels, validation data and labels, and a maximum number of epochs to train for. The function trains the model by looping over the number of epochs specified, performing a forward pass on the training data to compute the loss, computing the gradients concerning the loss, updating the weights and biases using the optimiser, and computing the loss on the validation data. Finally, it prints the training and validation loss at each epoch if the verbose flag is set to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d3b164e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, validation, and test sets\n",
    "X_train, X_val, X_test = np.split(X, [int(0.7 * len(X)), int(0.85 * len(X))])\n",
    "y_train, y_val, y_test = np.split(y, [int(0.7 * len(y)), int(0.85 * len(y))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539f1e08",
   "metadata": {},
   "source": [
    "This code above first splits the input data (X) and the labels (y) into three sets: a training set, a validation set, and a test set. It does this using the np.split function, which divides the input data into three equal-sized chunks based on the indices provided as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2f361da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the neural network\n",
    "nn = NeuralNetwork(input_size=784, output_size=10, output_type='multiclass_classification', num_classes=10, hidden_sizes=[128, 128], activations=['relu', 'relu'])\n",
    "# output_type parameter should be set to either 'binary_classification' or 'multiclass_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13405ef9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'NoneType' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31396\\1462419423.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Train the neural network without a validation set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31396\\2190250878.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, x_train, y_train, x_val, y_val, batch_size, learning_rate, max_epochs)\u001b[0m\n\u001b[0;32m    121\u001b[0m                 \u001b[1;31m# Compute the forward pass and the loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m                 \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m                 \u001b[0mbatch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m                 \u001b[0mepoch_train_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m                 \u001b[1;31m# Compute the backward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31396\\2190250878.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[1;34m(self, y_pred, y_true)\u001b[0m\n\u001b[0;32m    164\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbinary_cross_entropy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'multiclass_classification'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmulticlass_cross_entropy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'regression'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31396\\1600102473.py\u001b[0m in \u001b[0;36mmulticlass_cross_entropy_loss\u001b[1;34m(y_pred, y_true, num_classes)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;31m# Compute the multiclass cross entropy loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true_one_hot\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1e-8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'NoneType' and 'float'"
     ]
    }
   ],
   "source": [
    "# Train the neural network without a validation set\n",
    "history = nn.train(X_train, y_train, batch_size=64, learning_rate=1e-3, max_epochs=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c178a96",
   "metadata": {},
   "source": [
    " The code above trains the neural network using the train method of the nn object. This method takes a batch size, learning rate, and maximum number of epochs as arguments, and returns a history object containing information about the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "id": "324f4300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.NeuralNetwork at 0x220818729a0>"
      ]
     },
     "execution_count": 906,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 907,
   "id": "6f83265a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'NoneType' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13796\\4186413006.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Train the neural network with the training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13796\\2190250878.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, x_train, y_train, x_val, y_val, batch_size, learning_rate, max_epochs)\u001b[0m\n\u001b[0;32m    121\u001b[0m                 \u001b[1;31m# Compute the forward pass and the loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m                 \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m                 \u001b[0mbatch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m                 \u001b[0mepoch_train_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m                 \u001b[1;31m# Compute the backward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13796\\2190250878.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[1;34m(self, y_pred, y_true)\u001b[0m\n\u001b[0;32m    164\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbinary_cross_entropy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'multiclass_classification'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmulticlass_cross_entropy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'regression'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13796\\1600102473.py\u001b[0m in \u001b[0;36mmulticlass_cross_entropy_loss\u001b[1;34m(y_pred, y_true, num_classes)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;31m# Compute the multiclass cross entropy loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true_one_hot\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1e-8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'NoneType' and 'float'"
     ]
    }
   ],
   "source": [
    "# Define the input size, output size, and hidden sizes of the neural network\n",
    "input_size = 28 * 28  # MNIST images are 28x28 pixels\n",
    "output_size = 10  # MNIST has 10 classes\n",
    "hidden_sizes = [128, 64]  # Use two hidden layers with 128 and 64 units respectively\n",
    "\n",
    "# Define the activations for each layer\n",
    "activations = ['relu', 'relu', 'softmax']  # Use ReLU for the first two hidden layers and softmax for the output layer\n",
    "\n",
    "# Instantiate the neural network\n",
    "nn = NeuralNetwork(input_size=784, output_size=10, output_type='multiclass_classification', num_classes=10, hidden_sizes=[128, 128], activations=['relu', 'relu'])\n",
    "# output_type parameter should be set to either 'binary_classification' or 'multiclass_classification\n",
    "\n",
    "# Train the neural network with the training data\n",
    "history = nn.train(X_train, y_train, X_val, y_val, batch_size=batch_size, learning_rate=learning_rate, max_epochs=max_epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 908,
   "id": "f5c7899c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "[5 0 4 ... 2 7 0]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12738485",
   "metadata": {},
   "source": [
    "# Refrences\n",
    "\n",
    "Deep Neural Networks Backward Propagation (no date) PyLessons. Available at: https://pylessons.com/Deep-neural-networks-part3?fbclid=IwAR06KTI1P9wAn6bWywf_hNAG9NdHq9tiuEDpB5YFaFfggcMbetB-NDpWAVM. \n",
    "\n",
    "Calculating softmax in python (2021) AskPython. Available at: https://www.askpython.com/python/examples/calculating-softmax?fbclid=IwAR1T8IT3PKUfONe3s6shANpYzqK5wNaUh9G8d8ZQJvhmgh7zQhpzm67w7oI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9280abb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
